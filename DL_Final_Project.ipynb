{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5AWvmXnlIBjno6TAzfDCJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamaSamrin/DL-Final-Project/blob/main/DL_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading dataset**"
      ],
      "metadata": {
        "id": "tNPNiVfWoUHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from six.moves import urllib\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "DATA_DIRECTORY = \"data\"\n",
        "SOURCE_URL = 'http://35.183.27.27/'\n",
        "\n",
        "def maybe_download(filename):\n",
        "    \"\"\"Download the data, unless it's already here.\"\"\"\n",
        "    if not tf.io.gfile.exists(DATA_DIRECTORY):\n",
        "        tf.gfile.MakeDirs(DATA_DIRECTORY)\n",
        "    filepath = os.path.join(DATA_DIRECTORY, filename)     \n",
        "    if not tf.io.gfile.exists(filepath):\n",
        "        print('Start downloading dataset...')\n",
        "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)         \n",
        "        with tf.gfile.GFile(filepath) as f:\n",
        "            size = f.size()\n",
        "        print('Successfully downloaded', filename, size, 'bytes.')\n",
        "    return filepath\n",
        "\n",
        "def dense_to_one_hot(labels_dense, num_classes = 3):\n",
        "    num_labels = labels_dense.shape[0]\n",
        "    index_offset = np.arange(num_labels) * num_classes\n",
        "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
        "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
        "    return labels_one_hot\n",
        "\n",
        "def load_data(filename, one_hot=False):\n",
        "    with open(filename, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "        data = dict['data']\n",
        "        labels = dict['label']\n",
        "\n",
        "        if one_hot:\n",
        "            labels = dense_to_one_hot(labels)\n",
        "        return data, labels\n",
        "\n",
        "class DataSet(object):\n",
        "    def __init__(self, data, labels):\n",
        "        assert data.shape[0] == labels.shape[0], (\"data.shape: %s labels.shape: %s\" % (data.shape, labels.shape))\n",
        "        self._num_examples = data.shape[0]\n",
        "\n",
        "        self._data = data\n",
        "        self._lables = labels\n",
        "        self._epochs_completed = 0\n",
        "        self._index_in_epoch = 0\n",
        "\n",
        "    @property\n",
        "    def data(self):\n",
        "        return self._data\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        return self._lables\n",
        "\n",
        "    @property\n",
        "    def num_examples(self):\n",
        "        return self._num_examples\n",
        "\n",
        "    @property\n",
        "    def epochs_completed(self):\n",
        "        return self._epochs_completed\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        start = self._index_in_epoch\n",
        "        self._index_in_epoch += batch_size\n",
        "        if self._index_in_epoch > self._num_examples:\n",
        "            self._epochs_completed += 1\n",
        "            perm = np.arange(self._num_examples)\n",
        "            np.random.shuffle(perm)\n",
        "            self._data = self._data[perm]\n",
        "            self._lables = self._lables[perm]\n",
        "\n",
        "            start = 0\n",
        "\n",
        "            self._index_in_epoch = batch_size\n",
        "\n",
        "            assert batch_size <= self._num_examples\n",
        "        end = self._index_in_epoch\n",
        "        return self._data[start:end], self._lables[start:end]\n",
        "\n",
        "def read_data_sets(one_hot=False):\n",
        "    class DataSets(object):\n",
        "        pass\n",
        "    train_filename = maybe_download('train')\n",
        "    test_filename = maybe_download('test')\n",
        "\n",
        "    train_data, train_labels = load_data(train_filename, one_hot)\n",
        "    test_data, test_labels = load_data(test_filename, one_hot)\n",
        "    data_sets = DataSets()\n",
        "    data_sets.train = DataSet(train_data, train_labels)\n",
        "    data_sets.test = DataSet(test_data, test_labels)\n",
        "    return data_sets"
      ],
      "metadata": {
        "id": "-E2DgWYmemef"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HmkEzfmJ1Sjl"
      },
      "outputs": [],
      "source": [
        "# return DataSet class\n",
        "data = read_data_sets(one_hot=True)\n",
        "\n",
        "# get train data and labels by batch size\n",
        "train_x, train_label = data.train.next_batch(500)\n",
        "\n",
        "# get test data\n",
        "test_x = data.test.data\n",
        "\n",
        "# get test labels\n",
        "test_labels = data.test.labels\n",
        "\n",
        "# get sample number\n",
        "n_samples = data.train.num_examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwYvwRmeoOGd",
        "outputId": "bc330f09-d208-4521-89ca-94b64d17b6fe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[26.94933313 26.94942352 26.86116345 ... 16.22032493 17.53295088\n",
            "  17.53075379]\n",
            " [26.95540644 26.95561349 26.86818211 ... 16.22077216 17.53405894\n",
            "  17.53186146]\n",
            " [26.96394591 26.96354065 26.87468671 ... 16.22162589 17.53570343\n",
            "  17.53347345]\n",
            " ...\n",
            " [26.56521813 26.37605141 26.65220509 ... 16.86037896 18.29403388\n",
            "  18.29335327]\n",
            " [26.54994368 26.35584233 26.64187862 ... 16.85800073 18.29309627\n",
            "  18.29231912]\n",
            " [26.53391519 26.3387579  26.63412098 ... 16.85556478 18.29161395\n",
            "  18.2907373 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Next Step**"
      ],
      "metadata": {
        "id": "VLBRkXFvodCU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bNKkrrD-of9z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}